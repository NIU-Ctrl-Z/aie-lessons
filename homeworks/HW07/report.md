# HW07 – Report

## 1. Datasets

Выбраны все 4 датасета:

### 1.1 Dataset A 
- **Файл**: `S07-hw-dataset-01.csv`
- **Размер**: (12000, 8)
- **Признаки**: только числовые (8 признаков: `f01`–`f08`)
- **Пропуски**: нет
- **"Подлости"**: признаки в сильно разных шкалах (например, `f02` имеет std ≈ 60, а `f03` — std ≈ 0.5), что делает масштабирование критически важным.

### 1.2 Dataset B
- **Файл**: `S07-hw-dataset-02.csv`
- **Размер**: (8000, 3)
- **Признаки**: только числовые (`x1`, `x2`, `z_noise`)
- **Пропуски**: нет
- **"Подлости"**: нелинейная структура ("луны"), наличие выбросов, один шумовой признак (`z_noise`) с высокой дисперсией.

### 1.3 Dataset C
- **Файл**: `S07-hw-dataset-03.csv`
- **Размер**: (15000, 4)
- **Признаки**: только числовые (`x1`, `x2`, `f_corr`, `f_noise`)
- **Пропуски**: нет
- **"Подлости"**: кластеры разной плотности, фоновый шум, сложная геометрия.

### 1.4 Dataset D
- **Файл**: `S07-hw-dataset-04.csv` *(опциональный)*
- **Размер**: (10000, 32) → после препроцессинга: (10000, 42)
- **Признаки**: 30 числовых + 2 категориальных (`cat_a`, `cat_b`)
- **Пропуски**: есть (в числовых признаках)
- **"Подлости"**: высокая размерность, пропуски, категориальные переменные, риск «проклятия размерности».

---

## 2. Protocol

**Препроцессинг**:
- Все числовые признаки: `SimpleImputer(strategy='median')` -> `StandardScaler()`.
- Категориальные признаки (только в dataset-04): `OneHotEncoder(handle_unknown='ignore')`.
- Использован `ColumnTransformer` для корректного разделения потоков обработки.
- Все модели обучались на одинаково предобработанных данных.

**Поиск гиперпараметров**:
- **KMeans**: перебор `k = 2..20`, фиксированы `random_state=42`, `n_init="auto"` (или 10).
- **DBSCAN**: сетка `eps ∈ [0.3, 0.5, ..., 3.0]`, `min_samples ∈ [3, 5, 8, 10, 15]`.
- Выбор "лучшего" решения - по максимуму `silhouette_score`.

**Метрики**:
- Для всех моделей рассчитаны: `silhouette_score` (выше лучше), `davies_bouldin_score` (ниже лучше), `calinski_harabasz_score` (выше лучше).
- Для DBSCAN метрики считаются **только по non-noise точкам** (`label != -1`), доля шума явно указана.

**Визуализация**:
- PCA(2D) для лучшего решения на каждом датасете.
- График `silhouette vs k` для KMeans (сохранён в `artifacts/figures/`).

---

## 3. Models

Для каждого датасета сравнивались:
- **KMeans** с подбором `k` (2–20),
- **DBSCAN** с подбором `eps` и `min_samples`.

Опционально можно было использовать Agglomerative, но в данном решении использованы только KMeans и DBSCAN.

---

## 4. Results

### 4.1 Dataset A
- **Лучший метод и параметры**: KMeans (`k=2`)
- **Метрики**: silhouette = 0.522, Davies-Bouldin = 1.443, Calinski-Harabasz = 90.5
- **Комментарий**: DBSCAN дал высокую долю шума (>95%) и нестабильные кластеры. KMeans показал чёткое разделение на два компактных кластера.
- **Почему разумно**: данные после масштабирования имеют сферическую структуру; KMeans идеально подходит для таких случаев.

### 4.2 Dataset B
- **Лучший метод и параметры**: DBSCAN (`eps=0.7`, `min_samples=15`)
- **Метрики**: silhouette = 0.349, Davies-Bouldin = 1.892, Calinski-Harabasz = 120.3
- **Доля шума**: 0.051
- **Комментарий**: KMeans пытался разделить пространство линейно и получил низкий silhouette (~0.2). DBSCAN корректно выделил нелинейные "луны".
- **Почему разумно**: DBSCAN не предполагает сферичность и эффективно игнорирует разреженные выбросы.

### 4.3 Dataset C
- **Лучший метод и параметры**: KMeans (`k=3`)
- **Метрики**: silhouette = 0.316, Davies-Bouldin = 2.105, Calinski-Harabasz = 85.7
- **Комментарий**: DBSCAN давал либо слишком много шума, либо сливал кластеры. KMeans дал устойчивое решение с интерпретируемыми кластерами.
- **Почему разумно**: несмотря на разную плотность, кластеры остаются достаточно компактными; KMeans оказался более надёжным.

### 4.4 Dataset D
- **Лучший метод и параметры**: DBSCAN (`eps=2.0`, `min_samples=10`)
- **Метрики**: silhouette = 0.507, Davies-Bouldin = 1.210, Calinski-Harabasz = 210.4
- **Доля шума**: 0.488
- **Комментарий**: После кодирования категориальных признаков и импутации пропусков, DBSCAN смог выделить плотные группы. KMeans дал низкий silhouette (~0.35).
- **Почему разумно**: в условиях высокой размерности и смешанных типов признаков DBSCAN оказался более гибким.

---

## 5. Analysis

### 5.1 Сравнение алгоритмов
- **KMeans "ломается"** на данных с нелинейной структурой (dataset-02) и при наличии кластеров разной плотности (dataset-03), так как он предполагает сферичность и равную компактность.
- **DBSCAN выигрывает** там, где кластеры имеют произвольную форму или присутствуют выбросы (dataset-02, dataset-04).
- **Сильнее всего влияло на результат**:
  - масштабирование (без него KMeans работал некорректно),
  - наличие выбросов и нелинейной структуры,
  - высокая размерность и пропуски (в dataset-04).

### 5.2 Устойчивость
- Проверка проведена на **dataset-02** для KMeans (`k=2`): 5 запусков с разными `random_state`.
- Результаты показали высокую стабильность: средний `Adjusted Rand Index (ARI)` между всеми парами разбиений составил `0.997 ± 0.002`. Это означает, что несмотря на разные начальные условия, KMeans практически всегда воспроизводит одно и то же разбиение.
- **Вывод**: решение устойчиво, так как даже при разных начальных условиях KMeans воспроизводит практически идентичное разбиение.

---

## 6. Conclusion

1. **KMeans эффективен** только при сферических, компактных кластерах и требует обязательного масштабирования.
2. **DBSCAN мощен** для сложных форм и шумовых данных, но чувствителен к выбору `eps` и страдает при разной плотности кластеров.
3. **Препроцессинг — ключевой этап**: без `StandardScaler`, `SimpleImputer` и `OneHotEncoder` результаты были бы некорректными.
4. **Внутренние метрики помогают сравнивать**, но не заменяют визуализацию и предметную интерпретацию.
5. **PCA(2D) — необходимый инструмент** для понимания структуры кластеров в многомерных данных.
6. **Честный unsupervised-протокол** требует фиксации препроцессинга, гиперпараметров и критериев выбора.
7. **Устойчивость решения** — важный, но часто упускаемый аспект; её стоит проверять хотя бы на одном датасете.
8. **Нет универсального алгоритма**: выбор зависит от геометрии данных, наличия шума и бизнес-ограничений.